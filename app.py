
# ... ì´í•˜ ë™ì¼
# ...


# app.py
import streamlit as st
from prompt_templates import get_stage1_prompt, get_stage2_prompt
from llm_client import call_openai_chat
from tests.testset_loader import load_testset
import re

model_options = [
    "llama3.3",
    "llama4-scout",
    "llama4-maverick",
    "qwen2.5"
]

st.set_page_config(page_title="ğŸ§  System Prompt Optimizer (2ë‹¨ê³„ í¬í•¨)", layout="wide")
st.title("ğŸ§  Meta-Prompt ê¸°ë°˜ System Prompt ìµœì í™”ê¸°")

st.markdown("### ğŸ¯ Step 1: ì‚¬ìš©ì ì´ˆì•ˆ â†’ êµ¬ì¡°í™”ëœ ìµœì  í”„ë¡¬í”„íŠ¸")
st.info("""
Step 1ì—ì„œëŠ” ë‹¤êµ­ì–´ì™€ ë³µì¡í•œ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ì˜ ì²˜ë¦¬í•˜ëŠ” Qwen 2.5ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
""")

user_input = st.text_area("âœï¸ ì‚¬ìš©ì ì‘ì„± ì´ˆì•ˆ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸", height=200)

if 'stage1_output' not in st.session_state:
    st.session_state.stage1_output = None
if 'stage2_output' not in st.session_state:
    st.session_state.stage2_output = None

if st.button("ğŸ” 1ë‹¨ê³„ ìµœì í™” ì‹œì‘"):
    if not user_input.strip():
        st.warning("ì´ˆì•ˆ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("1ë‹¨ê³„ ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ìµœì í™” ì¤‘..."):
            stage1_prompt = get_stage1_prompt(user_input)
            try:
                result = call_openai_chat(stage1_prompt)
                match = re.search(r"<optimized_system_prompt>(.*?)</optimized_system_prompt>", result, re.DOTALL)
                optimized_prompt = match.group(1).strip() if match else result
                st.session_state.stage1_output = optimized_prompt
                st.success("âœ… 1ë‹¨ê³„ ìµœì í™” ì™„ë£Œ!")
                st.subheader("ğŸ“˜ ìµœì í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (1ë‹¨ê³„)")
                st.code(optimized_prompt, language="markdown")
            except Exception as e:
                st.error(f"1ë‹¨ê³„ ì‹¤íŒ¨: {e}")                

# 2ë‹¨ê³„ ì‹œì‘
if st.session_state.stage1_output:
    st.markdown("---")
    st.markdown("### ğŸ¤– Step 2: íƒ€ê²Ÿ LLM Self-Reflection ê¸°ë°˜ ìµœì í™”")
    target_model = st.selectbox("ğŸ§  íƒ€ê²Ÿ LLM ëª¨ë¸ ì„ íƒ", model_options, index=0)

    if st.button("âœ¨ 2ë‹¨ê³„ Self-Evolution ìˆ˜í–‰"):
        with st.spinner("2ë‹¨ê³„ Self-Reflection ìµœì í™” ì¤‘..."):
            stage2_prompt = get_stage2_prompt(
                st.session_state.stage1_output,
                target_model,
            )
            try:
                result2 = call_openai_chat(stage2_prompt)
                match2 = re.search(r"<final_model_specific_prompt>(.*?)</final_model_specific_prompt>", result2, re.DOTALL)
                final_prompt = match2.group(1).strip() if match2 else result2
                st.session_state.stage2_output = final_prompt
                st.success("âœ… 2ë‹¨ê³„ ìµœì í™” ì™„ë£Œ!")
                st.subheader(f"ğŸ§© {target_model} ì „ìš© ìµœì í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸")
                st.code(final_prompt, language="markdown")
                st.download_button("ğŸ“¥ ë‹¤ìš´ë¡œë“œ", final_prompt, file_name=f"{target_model}_final_prompt.txt")
            except Exception as e:
               st.error(f"2ë‹¨ê³„ ì‹¤íŒ¨: {e}")

if st.session_state.stage2_output:
    st.markdown("---")
    st.markdown("### ğŸ§ª Step 3: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ê¸°ë°˜ ìë™ í‰ê°€")

    if st.button("ğŸ“Š 3ë‹¨ê³„ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì‹¤í–‰"):
        with st.spinner("í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì‹¤í–‰ ì¤‘..."):
            try:
                testset = load_testset()
                results = []

                for idx, test in enumerate(testset, start=1):
                    sys_msg = {"role": "system", "content": st.session_state.stage2_output}
                    user_msg = {"role": "user", "content": test["input"]}
                    output = call_openai_chat([sys_msg, user_msg])
                    results.append({
                        "No": idx,
                        "ì§ˆë¬¸": test["input"],
                        "ì‘ë‹µ": output.strip(),
                        "ê¸°ì¤€": test["expected_criteria"]
                    })

                st.success("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
                for row in results:
                    st.markdown(f"**{row['No']}. ì§ˆë¬¸:** {row['ì§ˆë¬¸']}")
                    st.markdown(f"ğŸ“ **ê¸°ì¤€:** {row['ê¸°ì¤€']}")
                    st.code(row['ì‘ë‹µ'], language="markdown")
                    st.markdown("---")

                # í‰ê°€ ìš”ì²­
                if st.button("ğŸ¤– LLMìœ¼ë¡œ ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ìš”ì²­"):
                    with st.spinner("LLMì´ ì‘ë‹µ í’ˆì§ˆì„ í‰ê°€ ì¤‘..."):
                        review_prompt = "ì•„ë˜ì˜ ì‘ë‹µë“¤ê³¼ ê¸°ì¤€ì„ ì°¸ê³ í•´ ê° ì‘ë‹µì˜ í’ˆì§ˆì„ 1~5ì ìœ¼ë¡œ í‰ê°€í•˜ê³  JSONìœ¼ë¡œ ì •ë¦¬í•´ì¤˜:\n\n"
                        for r in results:
                            review_prompt += f"[ì§ˆë¬¸]: {r['ì§ˆë¬¸']}\n[ê¸°ì¤€]: {r['ê¸°ì¤€']}\n[ì‘ë‹µ]: {r['ì‘ë‹µ']}\n\n"
                        eval_result = call_openai_chat(review_prompt)
                        st.markdown("### ğŸ“ˆ ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ê²°ê³¼")
                        st.code(eval_result.strip(), language="json")
            except Exception as e:
                st.error(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")