
# ... ì´í•˜ ë™ì¼
# ...


# app.py
import streamlit as st
from prompt_templates import get_stage1_prompt, get_stage2_prompt
from llm_client import call_openai_chat
import re

model_options = [
    "llama3.3",
    "llama4-scout",
    "llama4-maverick",
    "qwen2.5"
]

st.set_page_config(page_title="ğŸ§  System Prompt Optimizer (2ë‹¨ê³„ í¬í•¨)", layout="wide")
st.title("ğŸ§  Meta-Prompt ê¸°ë°˜ System Prompt ìµœì í™”ê¸°")

st.markdown("### ğŸ¯ Step 1: ì‚¬ìš©ì ì´ˆì•ˆ â†’ êµ¬ì¡°í™”ëœ ìµœì  í”„ë¡¬í”„íŠ¸")
st.info("""
Step 1ì—ì„œëŠ” ë‹¤êµ­ì–´ì™€ ë³µì¡í•œ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ì˜ ì²˜ë¦¬í•˜ëŠ” Qwen 2.5ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
""")

user_input = st.text_area("âœï¸ ì‚¬ìš©ì ì‘ì„± ì´ˆì•ˆ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸", height=200)

if 'stage1_output' not in st.session_state:
    st.session_state.stage1_output = None
if 'stage2_output' not in st.session_state:
    st.session_state.stage2_output = None

if st.button("ğŸ” 1ë‹¨ê³„ ìµœì í™” ì‹œì‘"):
    if not user_input.strip():
        st.warning("ì´ˆì•ˆ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("1ë‹¨ê³„ ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ìµœì í™” ì¤‘..."):
            stage1_prompt = get_stage1_prompt(user_input)
            try:
                result = call_openai_chat(stage1_prompt)
                match = re.search(r"<optimized_system_prompt>(.*?)</optimized_system_prompt>", result, re.DOTALL)
                optimized_prompt = match.group(1).strip() if match else result
                st.session_state.stage1_output = optimized_prompt
                st.success("âœ… 1ë‹¨ê³„ ìµœì í™” ì™„ë£Œ!")
                st.subheader("ğŸ“˜ ìµœì í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (1ë‹¨ê³„)")
                st.code(optimized_prompt, language="markdown")
            except Exception as e:
                st.error(f"1ë‹¨ê³„ ì‹¤íŒ¨: {e}")                

# 2ë‹¨ê³„ ì‹œì‘
if st.session_state.stage1_output:
    st.markdown("---")
    st.markdown("### ğŸ¤– Step 2: íƒ€ê²Ÿ LLM Self-Reflection ê¸°ë°˜ ìµœì í™”")
    target_model = st.selectbox("ğŸ§  íƒ€ê²Ÿ LLM ëª¨ë¸ ì„ íƒ", model_options, index=0)

    if st.button("âœ¨ 2ë‹¨ê³„ Self-Evolution ìˆ˜í–‰"):
        with st.spinner("2ë‹¨ê³„ Self-Reflection ìµœì í™” ì¤‘..."):
            stage2_prompt = get_stage2_prompt(
                st.session_state.stage1_output,
                target_model,
            )
            try:
                result2 = call_openai_chat(stage2_prompt)
                match2 = re.search(r"<final_model_specific_prompt>(.*?)</final_model_specific_prompt>", result2, re.DOTALL)
                final_prompt = match2.group(1).strip() if match2 else result2
                st.session_state.stage2_output = final_prompt
                st.success("âœ… 2ë‹¨ê³„ ìµœì í™” ì™„ë£Œ!")
                st.subheader(f"ğŸ§© {target_model} ì „ìš© ìµœì í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸")
                st.code(final_prompt, language="markdown")
                st.download_button("ğŸ“¥ ë‹¤ìš´ë¡œë“œ", final_prompt, file_name=f"{target_model}_final_prompt.txt")
            except Exception as e:
               st.error(f"2ë‹¨ê³„ ì‹¤íŒ¨: {e}")